{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eef90c38-9ba2-4c24-82ee-437c77ebdf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad8d14a-d65e-41b4-89a6-0599e58533eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "class LInearRegresssion:\n",
    "     def __init__(self,learning_rate=0.01,n_iter=1000):\n",
    "         self.bias=None\n",
    "         self.weigths=None\n",
    "         self.lr=learning_rate\n",
    "         self.n_iter=n_iter\n",
    "  \n",
    "     def fit(self,x,y):#x_train,y_train\n",
    "        m,n=x.shape #number of samples,number of features\n",
    "\n",
    "        #steps 1\n",
    "        self.bias=0\n",
    "        self.weights=np.zeros(n) #[0,..........0]array of zero with length of number of feature\n",
    "\n",
    "         #gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            #step2\n",
    "            y_pred=self.bias+np.dot(x,self.weights)\n",
    "    \n",
    "            #step3\n",
    "            db=(1/m)*np.sum(y_pred-y)\n",
    "            dw=(1/m)*np.dot(x.T,(y_pred-y))\n",
    "    \n",
    "            #step4 convergence theorem\n",
    "            self.bias -=self.lr* db\n",
    "            self.weights -=self.lr*dw\n",
    "        \n",
    "\n",
    "     def predict(self,x):\n",
    "        \n",
    "        y_pred= self.bias+np.dot(x,self.weights)\n",
    "        return y_pred\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cff2dfb6-2f66-40ca-adc3-cdda2aa1e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[1],[2],[3],[4],[5]])\n",
    "y=np.array([2,4,6,8,10])\n",
    "\n",
    "\n",
    "model=LInearRegresssion()\n",
    "model.fit(x,y)\n",
    "\n",
    "y_pred=model.predict(x)\n",
    "print(y_pred)\n",
    "\n",
    "print(model.bias)\n",
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76ad096c-86cf-4c87-85fd-6b4e827f67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linerar reg with ols\n",
    "class linearRegression:\n",
    "    def __init__(self):\n",
    "        self.bias=None\n",
    "        self.weights=None\n",
    "\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        m,n=x.shape\n",
    "        x_b= np.c_[np.ones((m,1)),x]\n",
    "        \n",
    "        #normal equation\n",
    "        #theta=np.dot(np.linalg.inv(np.dot(x_b.T,x_b)),np.dot(x_b.T,y)) old way to write dot product\n",
    "        theta=np.linalg.inv(x_b.T@x_b)@x_b.T@y   #new way to write dot product\n",
    "        self.bias=theta[0]\n",
    "        self.weights=theta[1:]\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    def predict(self,x):\n",
    "        y_pred=self.bias+np.dot(x,self.weights)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82547a00-ef23-4cbf-a7d9-c771e270b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n",
      "5.329070518200751e-15\n",
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[1],[2],[3],[4],[5]])\n",
    "y=np.array([2,4,6,8,10])\n",
    "\n",
    "\n",
    "model=linearRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "y_pred=model.predict(x)\n",
    "print(y_pred)\n",
    "\n",
    "print(model.bias)\n",
    "print(model.weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
